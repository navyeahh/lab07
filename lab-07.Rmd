---
title: "Lab 07 - Modelling course evaluations"
author: "Navya Bhamidipati"
date: "`r Sys.Date()`"
output: html_document
---

### Packages and Data

```{r load-packages, message=FALSE, echo=TRUE}
library(tidyverse)
library(tidymodels)

```


```{r read-data}
evals<-read.csv("data/evals.csv", row.names=1)
```


# Exercise 1: Exploratory Data Analysis

1.  Visualize the distribution of `score` in the dataframe `evals`.

```{r viz-score}
ggplot(
  data = evals,
  mapping = aes(
    x = score
  )
) +
  geom_histogram()
```

*Yes, the data is negatively skewed. More people have given higher scores than lower ones. So, students rate the courses/professors higher than lower. That is what I expected to see because students might not want to risk giving their professors a lower rating if it may affect their grade in the future.*

2.  Visualize and describe the relationship between `score` and `bty_avg` using `geom_point()` to represent the data. 

```{r scatterplot}
ggplot(
  data = evals,
  mapping =
    aes(
      x = bty_avg,
      y = score
    )
  )+
  geom_jitter()
```

*Jitter moves the points in the dataset around by adding random variation to them. This helps us see the points with similar values e.g overplotting. The initial scatterplot did not show the overlapping points and gave the impression that there were very few entries. *

# Exercise 2: Simple Linear regression with a numerical predictor

1. Fit a linear model called `score_bty_fit` to predict average professor evaluation `score` from average beauty rating (`bty_avg`). Print the regression output using `tidy()`.

```{r fit-score_bty_fit}
# remove eval = FALSE from the code chunk options after filling in the blanks
score_bty_fit <- linear_reg() %>%
  set_engine("lm") %>%
  fit(score ~ bty_avg, data = evals)
```

```{r tidy-score_bty_fit, eval = FALSE}
# remove eval = FALSE from the code chunk options after filling in the blanks
tidy(score_bty_fit)
```

*score-hat = 3.88 + 0.067(bty_avg-hat).*

2. Plot the data again using `geom_jitter()`, and add the regression line.

```{r viz-score_bty_fit,eval=FALSE}
ggplot(
  data = evals,
  mapping = aes(
    x = bty_avg,
    y = score
  )
) +
  geom_jitter()+
  geom_smooth(method = lm)
```

3. Interpret the slope of the linear model in context of the data.

*For each change in one unit or one point of the average beauty score, the score is expected to change by 0.067 on average.  *

4. Interpret the intercept of the linear model in context of the data. Comment on whether or not the intercept makes sense in this context.

*For a beauty-average score of 0, the score is expected to be 3.88. This makes sense in this context because scores were calculated as the sum of beauty along with other teaching metrics. So , they could have a score of 0 on beauty but still score high in other parameters.*

5. Determine the $R^2$ of the model and interpret it in the context of the data.

```{r R2}
# remove eval = FALSE from the code chunk options after filling in the blanks
glance(score_bty_fit)$r.squared
```

*There is a slight positive correlation between beauty and score. The correlation coefficient (coefficient of determination) is 0.035.*

6. Make a plot of residuals vs. predicted values for the model above.

```{r viz-score_bty_fit-diagnostic, eval = FALSE}
# remove eval = FALSE from the code chunk options after filling in the blanks
score_bty_aug <- augment(score_bty_fit$fit)

ggplot(
  data = score_bty_aug,
  
) + ...
```

# Exercise 3: Simple Linear regression with a categorical predictor

0. Look at the variable rank, and determine the frequency of each category level.

```{r}
# ... 
```

1. Fit a new linear model called `score_rank_fit` to predict average professor evaluation `score` based on `rank` of the professor.

```{r fit-score_rank_fit}
# fit model

# tidy model output
```

*Add your narrative here.*

2. Fit a new linear model called `score_gender_fit` to predict average professor evaluation `score` based on `gender` of the professor. 

```{r fit-score_gender_fit}
# fit model

# tidy model output
```

```{r score_gender_intercept, eval=FALSE}
# remove eval = FALSE from the code chunk options
score_gender_intercept <- tidy(score_gender_fit) %>% 
  filter(term == "(Intercept)") %>%
  select(estimate) %>%
  pull()
```

```{r score_gender_slope, eval=FALSE}
# remove eval = FALSE from the code chunk options
score_gender_slope <- tidy(score_gender_fit) %>% 
  filter(term == "gendermale") %>%
  select(estimate) %>%
  pull()
```

*Add your narrative here. Use in-line code!*

# Exercise 4: Multiple linear regression

1. Fit a multiple linear regression model, predicting average professor evaluation `score` based on average beauty rating (`bty_avg`) and `gender.`

```{r fit-score_bty_gender_fit}
# fit model

# tidy model output
```

*Add your narrative here.*

```{r eval = FALSE}
ggplot(___) + ...
```

2. What percent of the variability in `score` is explained by the model `score_bty_gender_fit`. 

```{r}
# ...
```


3. What is the equation of the line corresponding to just male professors?

*Add your equation here.*

4. For two professors who received the same beauty rating, which gender tends to have the higher course evaluation score?

*Add your narrative here.*

5. How does the relationship between beauty and evaluation score vary between male and female professors?

*Add your narrative here.*

6. How do the adjusted $R^2$ values of `score_bty_fit` and `score_bty_gender_fit` compare? 

```{r eval=FALSE}
# remove eval = FALSE from the code chunk options after filling in the blanks
glance(___)$adj.r.squared
glance(___)$adj.r.squared
```

*Add your narrative here.*

7. Compare the slopes of `bty_avg` under the two models (`score_bty_fit` and `score_bty_gender_fit`).

*Add your narrative here.*

# Exercise 5: Interpretation of log-transformed response variables

If you do not know how to use LaTeX, do this exercise with pen and paper.
